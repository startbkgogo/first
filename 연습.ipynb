{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM+5Rr2CT9d9oFYCZ2xDYh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/startbkgogo/first/blob/main/%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM2HopLfy577",
        "outputId": "5db1b8d9-7ac4-4470-ceee-d27452312d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q langchain-google-genai\n",
        "!pip install -q langchain langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import time\n",
        "\n",
        "# --- 1. API 키 설정 ---\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    # Google Colab 등 환경에서는 직접 입력 대신 환경 변수 사용 권장\n",
        "    try:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Google API Key:\")\n",
        "    except Exception as e:\n",
        "        print(\"API 키를 환경 변수로 설정하거나 직접 입력해주세요.\")\n",
        "        print(f\"오류: {e}을\")\n",
        "        exit()\n",
        "\n",
        "# API 키가 설정되었는지 최종 확인\n",
        "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
        "    print(\"오류: GOOGLE_API_KEY가 설정되지 않았습니다.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"✅ Google API Key 설정 확인\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhFKVOarzlCK",
        "outputId": "3b4f7a66-0bb5-4e30-ff3a-4909a5df0252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key:··········\n",
            "✅ Google API Key 설정 확인\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 모델\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "# 벡터 DB\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# 문서 표현\n",
        "from langchain_core.documents import Document\n",
        "# 텍스트 크기 분할\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# 프롬프트 생성\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# 체인 내에서 데이터를 그대로 전달\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "# LLM 출력을 문자열로 변환\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "6kFqqMqHz7Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 샘플 데이터\n",
        "documents_data = [\n",
        "    \"대한민국의 수도는 서울입니다. 서울은 역사와 현대가 공존하는 도시입니다.\",\n",
        "    \"제주도는 아름다운 자연 경관으로 유명한 대한민국의 섬입니다. 한라산 국립공원이 대표적입니다.\",\n",
        "    \"부산은 대한민국 제2의 도시이자 최대 항구 도시입니다. 해운대 해수욕장이 유명합니다.\",\n",
        "    \"인공지능(AI)은 인간의 학습능력, 추론능력, 지각능력 등을 컴퓨터 프로그램을 통해 구현하는 기술입니다.\",\n",
        "    \"머신러닝은 인공지능의 한 분야로, 컴퓨터가 데이터를 통해 스스로 학습하고 예측하는 기술입니다.\"\n",
        "    \"1. 청년내일채움공제 \\\n",
        "[지원조건]\\\n",
        "중소기업에 정규직으로 취업한 청년에 해당하는 사람 [수령금액] 1,500만 원을 수령하게 됩니다.\"\n",
        "]"
      ],
      "metadata": {
        "id": "sEpdScXcz-HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 데이터를 Document 객체로 변환\n",
        "# page_context와 metadata를 가짐\n",
        "docs = [Document(page_content=text, metadata={\"source\": f\"doc_{i+1}\"})\n",
        "for i, text in enumerate(documents_data)]\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVIxvJOn0B5p",
        "outputId": "291c2145-8ca1-45ea-97ab-2fe6055a3b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'doc_1'}, page_content='대한민국의 수도는 서울입니다. 서울은 역사와 현대가 공존하는 도시입니다.'), Document(metadata={'source': 'doc_2'}, page_content='제주도는 아름다운 자연 경관으로 유명한 대한민국의 섬입니다. 한라산 국립공원이 대표적입니다.'), Document(metadata={'source': 'doc_3'}, page_content='부산은 대한민국 제2의 도시이자 최대 항구 도시입니다. 해운대 해수욕장이 유명합니다.'), Document(metadata={'source': 'doc_4'}, page_content='인공지능(AI)은 인간의 학습능력, 추론능력, 지각능력 등을 컴퓨터 프로그램을 통해 구현하는 기술입니다.'), Document(metadata={'source': 'doc_5'}, page_content='머신러닝은 인공지능의 한 분야로, 컴퓨터가 데이터를 통해 스스로 학습하고 예측하는 기술입니다.1. 청년내일채움공제 [지원조건]중소기업에 정규직으로 취업한 청년에 해당하는 사람 [수령금액] 1,500만 원을 수령하게 됩니다.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 검색 정확도를 높이고, 토큰을 효율적으로 활용하기 위해 chunk 단위로 분할\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200, # 각 청크의 크기\n",
        "    chunk_overlap = 20 # 청크 간 중복되는 글자 수\n",
        ")\n",
        "# split_documents -> 그냥 변수명\n",
        "# .split_documents() -> method\n",
        "split_documents = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"원본 문서 개수: {len(docs)}\")\n",
        "print(f\"분할된 청크 개수: {len(split_documents)}\")\n",
        "# print(\"\\n분할된 첫 번째 청크 예시:\")\n",
        "print(split_documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaVUtTA0Etp",
        "outputId": "8e56f91d-7bc3-42f4-e778-d32a6d5ff00e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 문서 개수: 5\n",
            "분할된 청크 개수: 5\n",
            "page_content='대한민국의 수도는 서울입니다. 서울은 역사와 현대가 공존하는 도시입니다.' metadata={'source': 'doc_1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 임베딩 모델 초기화\n",
        "# models/text_embedding-004 -> 최신 고성능 임베딩 모델\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "\n",
        "# llm 초기화\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash-latest\",\n",
        "    temperature=0.3 # 창의성\n",
        "    # 시스템 프롬프트를 사용자 프롬프트 형태로 변환하여 전달\n",
        "    # API 호환성과 model의 특성에 따라 유동적으로 선택하는 옵션\n",
        "    # convert_system_message_to_human=True\n",
        ")\n",
        "\n",
        "print(\"Gemini 임베딩 모델 및 LLM이 성공적으로 초기화되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkcbRnAN0Eq5",
        "outputId": "78b8f5e6-2eae-4cf5-892a-1968768f8c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini 임베딩 모델 및 LLM이 성공적으로 초기화되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 벡터 저장소 생성\n",
        "# Chorma.from_documents()는 모든 작업을 한 번에 수행\n",
        "# 1. 문서 청크를 임베딩 모델을 사용해 벡터로 변환\n",
        "# 2. 벡터와 원본 텍스트, 메타 데이터를 ChromaDB에 저장\n",
        "# persist_directory를 지정하면 해당 경로에 DB 파일이 저장됨. -> 재사용 가능\n",
        "vectorstore_path = \"./chroma_db_rag_store\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=split_documents, # 분할된 문서\n",
        "    embedding=embeddings, # 임베딩 모델\n",
        "    # persist : 지속하다\n",
        "    persist_directory=vectorstore_path # DB 저장할 디렉토리\n",
        ")\n",
        "\n",
        "# 저장된 DB를 다시 로드할 때 사용 (재생성 x)\n",
        "# vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)\n",
        "\n",
        "print(f\"문서들이 ChromaDB에 성공적으로 임베딩 및 저장되었습니다. (저장 경로: {vectorstore_path})\")\n",
        "# 기존의 RDB : table / Vector DB : collection\n",
        "print(f\"ChromaDB에 저장된 총 벡터(청크) 수: {vectorstore._collection.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7TtLzv70EmB",
        "outputId": "b5af15e6-0312-4738-895d-f2e0917212ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서들이 ChromaDB에 성공적으로 임베딩 및 저장되었습니다. (저장 경로: ./chroma_db_rag_store)\n",
            "ChromaDB에 저장된 총 벡터(청크) 수: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터 저장소에서 유사도 높은 문서를 검색하는 검색기 생성\n",
        "# retriever : 되찾다, 회수하다\n",
        "# k=3은 가장 유사한 문서 3개를 가져오도록 설정\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 검색기 테스트\n",
        "query_test = \"대한민국의 수도는 어디인가요?\"\n",
        "retrieved_docs_test = retriever.invoke(query_test)\n",
        "print(f\"'{query_test}'에 대한 검색 결과 (상위 {len(retrieved_docs_test)}개)\")\n",
        "for i, doc in enumerate(retrieved_docs_test):\n",
        "  # 메타데이터 중 source를 가져오되, 존재하지 않으면 기본 값을 가져옴\n",
        "  print(f\" {i+1}, {doc.page_content} (출처: {doc.metadata.get('source', 'N/A')})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-OJ30Xk0Ejp",
        "outputId": "89533204-35cb-4630-dfb3-b8040c0bee5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'대한민국의 수도는 어디인가요?'에 대한 검색 결과 (상위 3개)\n",
            " 1, 대한민국의 수도는 서울입니다. 서울은 역사와 현대가 공존하는 도시입니다. (출처: doc_1)\n",
            " 2, 제주도는 아름다운 자연 경관으로 유명한 대한민국의 섬입니다. 한라산 국립공원이 대표적입니다. (출처: doc_2)\n",
            " 3, 부산은 대한민국 제2의 도시이자 최대 항구 도시입니다. 해운대 해수욕장이 유명합니다. (출처: doc_3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG 파이프라인 구성 : 정보 검색 - 문맥 정보 준비 - 프롬프트 구성\n",
        "# - 답변 생성 및 출력 일련의 과정이 파이프라인\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "# {context} : 검색된 문서 내용, {question} : 사용자 질\n",
        "prompt_template = \"\"\"당신은 질문에 친절하고 상세하게 답변하는 AI 어시스턴트입니다.\n",
        "주어진 문맥(context) 정보를 바탕으로 질문에 답변해주세요. 문맥에서 답을 찾을 수 없다면, \"제공된 정보만으로는 답변하기 어렵습니다.\"라고 솔직하게 말해주세요.\n",
        "\n",
        "문맥:\n",
        "{context}\n",
        "\n",
        "질문:\n",
        "{question}\n",
        "\n",
        "답변:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# RAG 체인 정의\n",
        "# retriever로 문서를 검색하고\n",
        "# 검색된 문서들의 page_content를 하나의 문자열로 합\n",
        "# LCEL 사용 : |(파이프 기호)의 사용, 파이프라인 또는 체인을 더 쉽고 유연하게 구성하는 방식\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
        "    # RunnablePassthrough.assian() : 체인 내에서 새로운 키를 생성하고 값을 할당\n",
        "     # question에 초기 입력을 그대로 할당하기 위해 키:값으로 세팅\n",
        "     \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser() # llm 출력을 일반 문자열로 변환\n",
        ")\n",
        "\n",
        "print(\"RAG 체인이 성공적으로 구성되었습니다.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCKgOeHr0EhJ",
        "outputId": "8d427053-a6ed-4c12-9b93-f57eb0a3c82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG 체인이 성공적으로 구성되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG 체인에 질문을 입력하여 답변 생성\n",
        "\n",
        "question1 = \"대한민국의 수도는 어디이고, 그 도시는 어떤 특징이 있나요?\"\n",
        "print(f\"\\n[질문 1]: {question1}\")\n",
        "answer1 = rag_chain.invoke(question1)\n",
        "print(f\"[답변 1]: {answer1}\")\n",
        "\n",
        "question2 = \"AI 기술에 대해 설명해주세요.\"\n",
        "print(f\"\\n[질문 2]: {question2}\")\n",
        "answer2 = rag_chain.invoke(question2)\n",
        "print(f\"[답변 2]: {answer2}\")\n",
        "\n",
        "question3 = \"부산의 유명한 음식은 무엇인가요?\" # 샘플 문서에 없는 내용\n",
        "print(f\"\\n[질문 3]: {question3}\")\n",
        "answer3 = rag_chain.invoke(question3)\n",
        "print(f\"[답변 3]: {answer3}\")\n",
        "\n",
        "question4 = \"청년내일채움공제 지원조건 \" # 샘플 문서에 없는 내용\n",
        "print(f\"\\n[질문 4]: {question4}\")\n",
        "answer4 = rag_chain.invoke(question4)\n",
        "print(f\"[답변 4]: {answer4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evRUAZ2w0Eep",
        "outputId": "6588fc67-35a5-429e-9622-c4fff55ed642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[질문 1]: 대한민국의 수도는 어디이고, 그 도시는 어떤 특징이 있나요?\n",
            "[답변 1]: 대한민국의 수도는 서울입니다.  서울은 역사와 현대가 공존하는 도시입니다.\n",
            "\n",
            "[질문 2]: AI 기술에 대해 설명해주세요.\n",
            "[답변 2]: AI 기술은 인간의 학습능력, 추론능력, 그리고 지각능력을 컴퓨터 프로그램을 통해 구현하는 기술입니다.  즉, 인간이 지능적으로 수행하는 다양한 작업들을 컴퓨터가 수행할 수 있도록 하는 기술이라고 이해할 수 있습니다.  이는 단순한 명령어 수행을 넘어, 데이터를 학습하고, 새로운 상황에 적응하며, 스스로 판단하고 결정하는 능력을 컴퓨터에 부여하는 것을 목표로 합니다.\n",
            "\n",
            "[질문 3]: 부산의 유명한 음식은 무엇인가요?\n",
            "[답변 3]: 제공된 정보만으로는 답변하기 어렵습니다.  문맥에는 부산의 유명한 음식에 대한 정보가 포함되어 있지 않습니다.\n",
            "\n",
            "[질문 4]: 청년내일채움공제 지원조건 \n",
            "[답변 4]: 제공된 정보만으로는 답변하기 어렵습니다.  제공된 문맥은 대한민국의 서울, 제주도, 부산에 대한 간략한 설명일 뿐, 청년내일채움공제 지원조건에 대한 정보는 포함되어 있지 않습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJrnzocN0Eby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "448IsiTk0ERB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyUFAYYW0EAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}